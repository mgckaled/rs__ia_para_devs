{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando arquivo .env para controlar variaveis de ambiente\n",
    "Para evitar exposição da chave `OPENAI_API_KEY` optei por utilizar arquivo `.env` com a informação da chave.\n",
    "\n",
    "Para seguir o mesmo método basta criar um arquivo `.env` no mesmo diretório do arquivo `dataset_prep.ipynb`.\n",
    "A importação da chave será feita através da célula abaixo que faz a instalação de um biblioteca para carregar\n",
    "os valores do arquivo `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Dataset\n",
    "\n",
    "Neste vídeo, começamos um novo projeto no VS Code, criando um notebook Jupyter chamado \"Dataset prep.ipynb\". Importamos as bibliotecas necessárias e salvamos um arquivo com as versões das libs usando o comando \"pip freeze\". Utilizamos a biblioteca \"datasets\" para carregar um dataset diretamente do Hugging Face, removemos colunas desnecessárias e reduzimos o tamanho do dataset para 567 linhas, visando reduzir custos de treinamento. Essa abordagem pode impactar a qualidade do fine tuning, mas é útil para fins de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"hate-speech-portuguese/hate_speech_portuguese\", split='train[:10%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'hatespeech_G1', 'annotator_G1', 'hatespeech_G2', 'annotator_G2', 'hatespeech_G3', 'annotator_G3'],\n",
      "    num_rows: 567\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 567\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = datasets.remove_columns(['hatespeech_G1', 'annotator_G1', 'hatespeech_G2', 'annotator_G2', 'hatespeech_G3', 'annotator_G3'])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Neste trecho, explicamos como dividir um conjunto de dados em treino e teste para realizar o ajuste fino de um modelo de machine learning. Destacamos a importância de ter um conjunto de teste para avaliar o desempenho do modelo. Também abordamos a complexidade de construir manualmente um conjunto de dados para treinamento, ressaltando a necessidade de cuidado ao utilizar modelos pré-treinados para essa tarefa. A divisão do conjunto de dados em treino e teste é essencial para validar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A mídia DE NOTÍCIAS FALSAS desconhece a verdade deliberadamente. Grande perigo ao país. O @nytimes falido virou pia _ https://t.co/ssibMARmkc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando os Dados\n",
    "\n",
    "Neste trecho, é abordado o uso da função map para manipular um dataset. É criada uma função para remover caracteres específicos, como \"\\n\", e outra para alterar os rótulos das categorias. São feitas correções para garantir a execução correta das funções. Ao final, o dataset é modificado e exibido para verificar as alterações realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeN(example):\n",
    "    example[\"text\"] = example[\"text\"].replace(\"\\n\", \" \")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200563e4393f409f8c53db57ddb4c563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f317f284e8481a8c9e5e9019a65dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = datasets.map(removeN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A mídia DE NOTÍCIAS FALSAS desconhece a verdade deliberadamente. Grande perigo ao país. O @nytimes falido virou pia _ https://t.co/ssibMARmkc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label 0 -> No Hate Speech\n",
    "### label 1 -> Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelChange(example):\n",
    "    example[\"label_text\"] = \"No Hate Speech\" if  example[\"label\"] == 0 else \"Hate Speech\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bdf65e736f648dcb1d45684545cfda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e495b354934034a2a91a5f97e38099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 453\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 114\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = datasets.map(labelChange)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label_text'],\n",
       "        num_rows: 453\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label_text'],\n",
       "        num_rows: 114\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = datasets.remove_columns(['label'])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'A mídia DE NOTÍCIAS FALSAS desconhece a verdade deliberadamente. Grande perigo ao país. O @nytimes falido virou pia _ https://t.co/ssibMARmkc',\n",
       " 'label_text': 'No Hate Speech'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizando Dataset\n",
    "\n",
    "Estamos quase finalizando nosso dataset no formato desejado para a OpenAI. Vamos estruturar as mensagens com os papéis de sistema, usuário e assistente, garantindo que o conteúdo do usuário seja o texto e a resposta do assistente seja a classificação de discurso de ódio. Criaremos uma função em Python para transformar nosso dataset em um arquivo JSONL, que é o que a OpenAI espera. Também discutimos a importância do fine-tuning e os custos envolvidos nesse processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTRUÇÃO DO OBJETO PARA OPEN AI\n",
    "def dataset_to_json(datasets, file_name):\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as f:\n",
    "        for example in datasets:\n",
    "            json_obj = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"Seu trabalho é classificar os comentários do usuário em Hate Speech ou No Hate Speech.\"},\n",
    "                    {\"role\": \"user\", \"content\": example[\"text\"]},\n",
    "                    {\"role\": \"assistant\", \"content\": example[\"label_text\"]}\n",
    "                ]\n",
    "            }\n",
    "            f.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning GPT OpenAI\n",
    "\n",
    "Nesta aula, vamos finalizar a construção do Fine Tuning utilizando a API da OpenAI. Começamos importando as bibliotecas necessárias e conectando nossa chave de API. Em seguida, fazemos o upload dos arquivos de treinamento e validação, especificando seus propósitos. Após isso, iniciamos o processo de Fine Tuning com o modelo GPT-3.5 Turbo. Acompanhe o progresso no dashboard da OpenAI e, em breve, veremos os resultados do nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = client.files.create(\n",
    "    file=open(\"train.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_validation = client.files.create(\n",
    "    file=open(\"validation.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-WJ260o5JFtFYO2Lc56LC6Xi0', created_at=1737333203, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-9Ka4MdZAwINdpOMmxZxue32b', result_files=[], seed=568076911, status='validating_files', trained_tokens=None, training_file='file-Pq8eAq7LNP69yb8nPerGxF', validation_file='file-UMJvtV1xTSft5WkTbkND1N', estimated_finish=None, integrations=[], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size='auto', learning_rate_multiplier='auto', n_epochs='auto')), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Devido ao Fine-tune ter sido descontinuado para modelo davinci-002 estarei utilizando o modelo mais recente até momento de saída da aula e também recomendado pela documentação\n",
    "# https://platform.openai.com/docs/guides/fine-tuning#create-a-fine-tuned-model\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=file_train.id,\n",
    "    validation_file=file_validation.id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_json_aws(datasets, file_name):\n",
    "    with open(file_name, 'w', encoding=\"utf-8\") as f:\n",
    "        for example in datasets:\n",
    "            json_obj = {\n",
    "                \"prompt\": example[\"text\"],\n",
    "                \"completion\": example[\"label_text\"]\n",
    "            }\n",
    "            f.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_json_aws(datasets[\"train\"], \"train_aws.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_json_aws(datasets[\"test\"], \"validation_aws.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_para_devs-NJoqudvH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
