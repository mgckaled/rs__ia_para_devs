#!/usr/bin/env python
# coding: utf-8

import json
import os

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_core.documents.base import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence
from langchain_core.vectorstores.base import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Load dos modelos (Embeddings e LLM)
embeddings_model = OpenAIEmbeddings()
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    max_tokens=200,

)

# Funcoes


def load_data() -> VectorStoreRetriever:
    # Carregar o PDF
    """
    Loads a PDF document, splits it into chunks, and creates a retriever for querying.

    This function performs the following steps:
    1. Loads a PDF document from a specified path, excluding images.
    2. Splits the document into smaller text chunks using a character-based text splitter.
    3. Converts the text chunks into a vector database using an embedding model.
    4. Returns a retriever configured to search for the top 3 most relevant document chunks.

    Returns:
        The retriever object for querying the vector database.
    """

    pdf_link = "DOC-SF238339076816-20230503.pdf"
    loader = PyPDFLoader(pdf_link, extract_images=False)
    pages = loader.load_and_split()

    # Separar em Chunks (Pedaços de documento)
    text_spliter = RecursiveCharacterTextSplitter(
        chunk_size=4000,
        chunk_overlap=20,
        length_function=len,
        add_start_index=True,
    )

    chunks = text_spliter.split_documents(pages)
    vectordb = Chroma.from_documents(chunks, embedding=embeddings_model)

    # Load Retriever
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    return retriever


def get_relevant_docs(question) -> list[Document]:
    """
    Returns relevant documents (chunks) given a question.

    Args:
        question (str): The question to be answered.

    Returns:
        list: A list of relevant document chunks.
    """
    retriever = load_data()
    context = retriever.invoke(question)
    return context


def ask(question, llm):
    """
    Generates a response to a given question using a language model (LLM) and relevant context.

    Args:
        question (str): The question to be answered.
        llm (Callable): The language model used for generating the response.

    Returns:
        str: The response generated by the LLM based on the provided context and question.
    """

    template_cq = """
    Você é um especialista em legistalação e tecnologia. Responda a pergunta abaixo utilizando o contexto informado

    Contexto: {context}
    
    Pergunta: {question}
    """
    prompt = PromptTemplate(
        input_variables=["context", "question"], template=template_cq)
    sequence = RunnableSequence(prompt | llm)
    context = get_relevant_docs(question)
    response = sequence.invoke({"context": context, "question": question})
    return response


def lambda_handler(event, context):
    """
    Lambda handler to process a given question and return a relevant answer.

    The function will parse the event body, extract the question, ask the LLM for a response and return a JSON object with the response.

    Args:
        event (dict): The event object passed to the Lambda function.
        context (dict): The Lambda context object.

    Returns:
        dict: A JSON object containing the response message and details.
    """
    body = json.loads(event.get("body", {}))
    query = body.get("question")
    response = ask(query, llm).content

    return {
        "statusCode": 200,
        "headers": {
            "Content-Type": "application/json"
        },
        "body": json.dumps({
            "message": "Tarefa Concluída com sucesso",
            "details": response,
        })
    }
