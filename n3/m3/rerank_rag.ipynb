{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando arquivo .env para controlar variaveis de ambiente\n",
    "Para evitar exposição da chave `OPENAI_API_KEY` optei por utilizar arquivo `.env` com a informação da chave.\n",
    "\n",
    "Para seguir o mesmo método basta criar um arquivo `.env` no mesmo diretório do arquivo `rerank_rag.ipynb`.\n",
    "A importação da chave será feita através da célula abaixo que faz a instalação de um biblioteca para carregar\n",
    "os valores do arquivo `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando arquivo\n",
    "\n",
    "Vamos importar as libs e definir a chave da API da OpenAI. Em seguida, importaremos os modelos de embeddings e chat da OpenAI. O modelo de embeddings será o \"text-embedding-3-small\" e o modelo de chat será o \"gpt-3.5-turbo\". Limitaremos a quantidade de tokens de saída com o parâmetro \"max-tokens\". Carregaremos um PDF da pasta root e usaremos o PyPDF para separar as páginas do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelos Open AI - Embedding e chat\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o PDF\n",
    "pdf_link = \"DOC-SF238339076816-20230503.pdf\"\n",
    "loader = PyPDFLoader(pdf_link, extract_images=False)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total de paginas do arquivo .pdf\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando os Chunks\n",
    "\n",
    "[Documentação sobre ajuste no banco de dados](https://docs-rocketseat.notion.site/Ajuste-aula-de-Rerank-193395da5770808aba49c3f49914ec2c)\n",
    "\n",
    "Vamos separar o PDF em chunks usando o text splitter com chunk size de 4000 e overlap de 20. Criamos a função de splitter e aplicamos o text_splitter.split() nas páginas do documento. Em seguida, salvamos os chunks no vectorDB, utilizando o embedding model e o diretório NaiveDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar em chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os Chunks no VectorDB\n",
    "vectordb = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"naiveDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Rerank retriever\n",
    "\n",
    "Neste trecho da aula, foi abordado o processo de carregamento do banco de dados e a criação do NaiveRetriever, que é a primeira etapa do Contextual Compression RAG. O instrutor explicou a importância de buscar mais documentos para permitir um ranqueamento mais preciso. Em seguida, demonstrou como acessar o Cohere Rerank no Google e criar uma API Key gratuita para utilização. Por fim, mostrou como construir o Compression Retriever, que utiliza o Base Compressor e o Base Retriever para otimizar o ranqueamento dos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o DB\n",
    "naive_retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank = CohereRerank(model=\"rerank-v3.5\", top_n=3)\n",
    "\n",
    "compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=rerank,\n",
    "    base_retriever=naive_retriever,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o Prompt e Chain\n",
    "\n",
    "Neste resumo, foi abordado o processo de criação de um template para responder perguntas sobre legislação e tecnologia. Foi mencionado o uso de um prompt, a configuração da recuperação de informações e a criação de uma cadeia de processamento. Houve um problema com a coleta de documentos devido a uma alteração no nome de um banco de dados, que foi corrigido. Ao final, foi mostrado o resultado da resposta utilizando o método do Contextual Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# especificar o prompt com pergunta e contexto\n",
    "TEMPLATE = \"\"\"\n",
    "    Você é um especialista em legislação e tecnologia. Responda a pergunta abaixo utilizando o contexto informado.\n",
    "    Query:\n",
    "    {question}\n",
    "    Context:\n",
    "    {context}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_retrival = RunnableParallel({\"question\": RunnablePassthrough(), \"context\": compressor_retriever})\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "compressor_retrieval_chain = setup_retrival | rag_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response e conclusão\n",
    "\n",
    "Neste resumo, foi abordado o processo de criação de um sistema de busca e ranqueamento de documentos. Foram mencionadas etapas como a criação do banco de dados, importação de modelos, separação em chunks, criação do Reranker e do Compressor Retriever. Também foi destacada a utilização de um modelo externo para o reranqueamento. Na próxima aula, será abordado o deploy de um modelo de RAG como serviço de API para integração em aplicações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como não há um contexto específico fornecido, posso destacar alguns dos principais riscos do marco legal de IA (Inteligência Artificial) que geralmente são discutidos:\\n\\n1. Violação da privacidade: O uso de IA pode envolver a coleta e processamento de grandes quantidades de dados pessoais, o que pode resultar em violações de privacidade se não houver regulamentações adequadas para proteger essas informações.\\n\\n2. Discriminação algorítmica: Algoritmos de IA podem reproduzir e amplificar preconceitos existentes, resultando em decisões discriminatórias em áreas como contratação, empréstimos, justiça criminal, entre outros.\\n\\n3. Responsabilidade e transparência: A complexidade dos sistemas de IA pode dificultar a atribuição de responsabilidade em caso de erros ou danos causados \\u200b\\u200bpor decisões automatizadas. Além disso, a falta de transparência nos algoritmos pode dificultar a compreensão de como as decisões são tomadas.\\n\\n4. Segurança cibernética: A integração de sistemas de IA em redes e infraestruturas pode aumentar a vulnerabilidade a ataques cibernéticos, exigindo medidas de segurança robustas para proteger os sistemas.\\n\\n5. Impactos socioeconômicos: A automação impulsionada pela IA pode levar à substituição de empregos e desigualdade econômica, exigindo políticas e estratégias para mitigar esses impactos negativos.\\n\\nPortanto, é crucial que o marco legal de IA aborde esses riscos e estabeleça regulamentações claras para garantir o uso ético e responsável da tecnologia.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressor_retrieval_chain.invoke(\"Quais os principais riscos do marco legal de ia\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_para_devs-NJoqudvH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
